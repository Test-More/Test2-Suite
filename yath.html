<dl class="listnav">
    <dt id="overview">Overview</dt>
    <dd>
        <h2>Yath (Yet Another Test Harness / Test2::Harness)</h2>

        <p>

        Yath is a new alternative to prove that allows you to take full
        advantage of Test2.

        <p>

        Brief list of features:
        <ul>
            <li>Complete event logging</li>
            <li>Better output for humans (Not TAP)</li>
            <li>Bypasses TAP completely</li>
            <li>Can preload and fork for each tests</li>
            <li>Advanced preload options</li>
            <li>Powerful plugin hooks</li>
            <li>Can have a persistant instance</li>
        </ul>
    </dd>

    <dt id="Output">Output</dt>
    <dd>
        <dl class="sub_list">
            <dt id="normal">Normal</dt>
            <dd>
                <h2>Normal Output</h2>

                <br />

                Here is a run with multiple tests, one of which fails:

                <br />
                <img src="yath-normal.png" style="display: block; margin-top: 6px; max-width: 90%;">

                <br />
                <p>

                While tests are actively running yath will maintain a status at the bottom of all output that looks like this:

                <br />
                <img src="yath-tail.png" style="display: block; margin-top: 6px; max-width: 90%">
            </dd>

            <dt id="verbose">Verbose</dt>
            <dd>
                <h2>Verbose Output</h2>

                <p>
                In verbose mode you see all test-generated events, each on their own line (or multiple lines)

                <img src="yath-verbose.png" style="display: block; margin-top: 6px; max-width: 90%">
            </dd>

            <dt id="noisy">Noisy</dt>
            <dd>
                <h2>Very Verbose Output</h2>

                <p>

                You can always increase verbosity by adding another 'v' to
                '-vv'. Doing so will cause extra information from the harness
                to be displayed:

                <img src="yath-vv.png" style="display: block; margin-top: 6px; max-width: 90%;">
            </dd>
        </dl>
    </dd>

    <dt id="Logging">Logging</dt>
    <dd>
        <dl class="sub_list">
            <dt id="Log">Log</dt>
            <dd>
                <h2>Event Logging</h2>

                <p>

                You can keep a log of all events using the '-L' (Log), '-G'
                (Gzipped log) or '-B' (Bzip2 log) flags. '-B' is recommended as
                these logs can be quite large. Yath will tell you where to find
                the log at the end:

                <script class="output">
                All tests were successful!

                Wrote log file: /.../test-logs/2017-10-24~12:52:20~1508874740~24073.jsonl.bz2
                </script>

                <p>

                The log is a jsonl file with every single event generated during the test run, in order they were seen.
            </dd>

            <dt id="Replay">Replay</dt>
            <dd>
                <h2>Replay from a log</h2>

                <p>

                When you replay from a log, the log is used as the source of
                events. No tests are actually run. The original source does not
                need to be present, the log file is all you need.

                <p>

                One major benefit of being able to replay a log is that you can
                use a different verbosity, or even a different output format.
                You can also tell it to only show you specific test jobs.

                <p>

                <script class="output">
                    $ yath replay -v log_file.jsonl.bz2 11 34 42
                </script>

                The above command will replay events from the specified log,
                but will only show jobs 11, 34, and 42. Notice that '-v' was
                used, so we will have verbose output. The original run may not
                have been verbose, it does not matter.
            </dd>

            <dt id="Times">Times</dt>
            <dd>
                <h2>The 'times' command</h2>

                The 'times' command will analize a log file and tell you how much time was spent in each test file:

                <script class="output">
exodist@abydos main $ yath times log_file.jsonl.bz2
+----------+----------+----------+----------+---------------+
| Total    | Startup  | Events   | Cleanup  | File          |
+----------+----------+----------+----------+---------------+
| 0.11451s | 0.08027s | 0.00140s | 0.03284s | t/bad_test.t  |
| 0.11801s | 0.08711s | 0.00025s | 0.03065s | t/good_test.t |
| 2.11867s | 0.08747s | 2.00091s | 0.03029s | t/long_test.t |
| --       | --       | --       | --       | --            |
| 2.35119s | 0.25485s | 2.00256s | 0.09378s | TOTAL         |
+----------+----------+----------+----------+---------------+
                </script>

                <ul>
                    <li>Startup is the time between launch and the first 'primary' event.</li>
                    <li>Events is time spent between the first and last 'primary' event.</li>
                    <li>Cleanup is time spent between the last 'primary' event and exit.</li>
                </ul>

                Note: Some of this data is best-estimate, for instance
                'exit' is when the harness noticed the test was done, not
                necessarily the exact moment the test finished. If a test is
                bypassing the Stream formatter and outputting TAP directly then
                all the numbers could be wrong.

                <p>
                Note: Primary events are assertions, plans, and similar state-changing events.
            </dd>
        </dl>
    </dd>

    <dt id="Preload">Preload</dt>
    <dd>
        <dl class="sub_list">
            <dt id="Simple">Simple</dt>
            <dd>
                <h2>Simple Preloading</h2>

                <p>

                You can preload any module with the '-PModule' option. You can specify any number of preloads.

                <p>

                Example, preload Moose before running all tests:
                <script class="output">
                    $ yath -PMoose
                </script>

                <p>

                Moose test suite time without preloading Moose (prove has similar numbers)

                <script class="output">
                01m:35.56s on wallclock (3.53 usr 0.27 sys + 84.55 cusr 7.03 csys = 95.38 CPU)
                </script>

                <p>

                Moose test suite time preloading Moose first:

                <script class="output">
                40.2088s on wallclock (2.99 usr 0.26 sys + 28.87 cusr 6.63 csys = 38.75 CPU)
                </script>
            </dd>

            <dt id="Advanced">Advanced</dt>
            <dd>
                <h2>Advanced Preloading</h2>

                <p>

                You can create preload modules with advanced behavior. If the
                module you preload subclasses Test2::Harness::Preload it will
                get special treatment.

                <p>

                <ul>
                    <li>Optional OO - If you provide a new() method then your preload will be instanciated.</li>
                    <li>preload hook - called when loaded</li>
                    <li>pre_fork hook - called before forking for tests</li>
                    <li>post_fork hook - called right after forking, in the test</li>
                    <li>pre_launch hook - called just before the harness drops down to the test</li>
                </ul>

                You can also define stages. An example use of stages is when
                you want to preload multiple sets of conflicting modules. You
                can load each set independantly (or one after the other if you
                want to build on top of the previous one). Tests can be marked
                to run in specific stages.
            </dd>
        </dl>
    </dd>

    <dt id="Project">Project</dt>
    <dd>
        <dl class="sub_list">
            <dt id="Init">Init</dt>
            <dd>
                <h2>Init command</h2>

                <p>

                The 'init' command is a simple command designed for cpan module authors.

                <p>

                This command generates a 'test.pl' file that will execute all
                tests under the 't' and 't2' directories using yath.

                <p>

                'test.pl' is executed by most build tools when tests are run.
                The difference between test.pl and other test files is that it
                is not run via Test::Harness, and as such only the exit value
                matters. This is the easiest way to make sure people using your
                cpan modules will have the tests run by the correct harness.
            </dd>

            <dt id="Config">Config</dt>
            <dd>
                <h2>Configure with .yath.rc file</h2>

                <p>

                A .yath.rc file in your project root will provide the default
                arguments for yath commands. Each command gets a section, and
                each item in that section is passed to yath as an argument
                BEFORE any command line arguments. Many options can be negated
                on the command line if needed, so .yath.rc simply defines the
                defaults.

                <script class="output">
                [test]
                -Iextra/libs          ; we need these extra libs
                -B                    ; Always keep a log
                --default_search 't2' ; Only search the t2 directory by default

                [start]
                -Iextra/libs ; we need these extra libs

                [run]
                -B                    ; Always keep a log
                --default_search 't2' ; Only search the t2 directory by default
                </script>
            </dd>
        </dl>
    </dd>

    <dt id="Directives">Directives</dt>
    <dd>
        <dl class="sub_list">
            <dt id="About">About</dt>
            <dd>
                <h2>What are directives?</h2>

                <p>

                Directives are comment you can put in the header of your test
                file. These comments can control how the harness runs or
                processes your test.

                <p>

                These comments should appear before any run-time statement.
                They may follow the shbang (#!) line, and any use statements,
                or 1-line BEGIN blocks.

                <p>


                <script class="code">
                #!/usr/bin/perl
                use strict;
                use warnings;

                # HARNESS-DIRECTIVE-1
                # HARNESS-DIRECTIVE-2
                # HARNESS-DIRECTIVE-3

                ...
                </script>
            </dd>

            <dt id="Timeout">Timeout</dt>
            <dd>
                <h2>Timeout Directives</h2>

                <p>

                <script class="code">
                # HARNESS-NO-TIMEOUT
                </script>

                This tells the harness that it should not timeout when running this test. <b>Not recommended</b>.

                <p>

                <script class="code">
                # HARNESS-TIMEOUT-EVENT 90
                </script>

                This tells the harness that it should not timeout on events in
                less than 90 seconds. By default a test will timeout if no
                events are recieved for 60 seconds, if you know your test may
                take longer than that you can set this.

                <script class="code">
                # HARNESS-TIMEOUT-POSTEXIT 30
                </script>

                If a test exits true, but does not appear to be really finished
                (maybe it forked a child), the harness will wait up to 15
                seconds for more events to come in. If you know it will take
                longer than this you can use this directive.
            </dd>

            <dt id="Preload">Preload</dt>
            <dd>
                <h2>Preload</h2>

                <p>

                <script class="code">
                # HARNESS-NO-PRELOAD
                </script>

                This tells the harness that this test cannot be run with
                preloads. The harness will instead execute the test using
                IPC::Open3 to run it with a completely new interpreter
                instance.

                <p>

                At the moment there is no difference between this and the
                HARNESS-NO-FORK directive. That may change.
            </dd>

            <dt id="Fork">Fork</dt>
            <dd>
                <h2>Fork</h2>

                <p>

                <script class="code">
                # HARNESS-NO-FORK
                </script>

                This tells the harness that this test cannot be run by forking
                (the default). The harness will instead execute the test using
                IPC::Open3 to run it with a completely new interpreter
                instance.

                <p>

                At the moment there is no difference between this and the
                HARNESS-NO-FORK directive. That may change.
            </dd>

            <dt id="Stream">Stream</dt>
            <dd>
                <h2>Stream</h2>

                <p>

                <script class="code">
                # HARNESS-NO-STREAM
                </script>

                <p>

                This tells the harness that the test will break if
                Test2::Formatter::Stream is used instead of the TAP formatter.

                <p>

                Use this if your test mixes regular tools and hard-coded TAP.
                <b>Note: DO NOT DO THIS!</b>
            </dd>

            <dt id="Category">Category</dt>
            <dd>
                <h2>Category</h2>

                <p>

                The category is used when sorting tests (but only with a -j
                setting &gt; 1). The primary goal is to make sure to start running
                long tests first in n-1 slots, while running other tests in the
                remaining slot.

                <p>

                This is the default, no need to actually specify it:
                <script class="code"># HARNESS-CAT-DEFAULT</script>
                <p>

                This is the default for any test with NO-FORK or NO-PRELOAD specified.
                <script class="code"># HARNESS-CAT-MEDIUM</script>
                <p>

                Use this to mark tests that are long.
                <script class="code"># HARNESS-CAT-LONG</script>
                <p>

                Use this to mark tests that should not be run concurrently with
                other IMMISCIBLE tests.
                <script class="code"># HARNESS-CAT-IMMISCIBLE</script>
                <p>

                Use this to mark a test that cannot be run concurrently with
                any other tests.
                <script class="code"># HARNESS-CAT-ISOLATION</script>
            </dd>

            <dt id="Stage">Stage</dt>
            <dd>
                <h2>Stages</h2>

                <p>

                The stage directives work along with preload stages. They let
                you tell the harness which preload stage should run the test.
                If you specify an invalid stage, the test will run under the
                'DEFAULT' stage.

                <p>

                <script class="code"># HARNESS-STAGE-FOO</script>
            </dd>
        </dl>
    </dd>

    <dt id="Persist">Persist</dt>
    <dd>
        <dl class="sub_list">
            <dt id="Start">Start</dt>
            <dd>
                <h2>The 'start' command</h2>

                <p>

                This command is used to start a new persistent yath instance.

                <p>

                <script class="output">
$ yath start -j2 -PMoose

Waiting for runner...

Persistent runner started!
Runner PID: 31556
Runner dir: /dev/shm/yath-test-31555-M3Jtq7hd
Runner logs:
  standard output: /dev/shm/yath-test-31555-M3Jtq7hd/output.log
  standard  error: /dev/shm/yath-test-31555-M3Jtq7hd/error.log

Use `yath watch` to monitor the persistent runner

0.08120s on wallclock (0.14 usr 0.01 sys + 0.00 cusr 0.00 csys = 0.15 CPU)
                </script>

                The above example will start a persistent yath instance with 2
                jobs slots. The instance will preload Moose.
            </dd>

            <dt id="Run">Run</dt>
            <dd>
                <h2>The 'run' command</h2>

                'run' is the same as 'test' except it hands the jobs off to a
                persistent runner instead of starting a new runner.

                <script class="output">
$ yath run t/basics/create.t

( PASSED )  job 31632-1    t/basics/create.t

================================================================================

Run ID: 1508881606

All tests were successful!
                </script>

Note: The job-id will be the process of the 'run' command followed by a
sequential number.
            </dd>

            <dt id="Which">Which</dt>
            <dd>
                <h2>The 'which' command</h2>

                <p>

                This command tells you where the persistant instance lives (or
                if there is none).

                <p>

                <script class="output">
$ yath which

Found: /home/exodist/projects/Moose-2.2006/.yath-persist.json
  PID: 31625
  Dir: /dev/shm/yath-test-31624-_hCik49C
                </script>

            </dd>

            <dt id="Watch">Watch</dt>
            <dd>
                <h2>The 'watch' command</h2>

                <p>

                This command lets you monitor the output of a persistent
                runner. This will only show you output produced by the runner,
                it will not show any test results.

                </p>

                <script class="output">
$ yath watch

Found: /home/exodist/projects/Moose-2.2006/.yath-persist.json
  PID: 31625
  Dir: /dev/shm/yath-test-31624-_hCik49C

31625 (default) Ready to run tests...
31625 (default) Runner cought SIGHUP, reloading...
31625 (default) Waiting for currently running jobs to complete before exiting for restart...
31625 (default) Waiting for currently running jobs to complete before respawning...
31625 (default) Ready to run tests...
                </script>

                Note: STDOUT and STDERR and mixed here, so things may appear out of order.
            </dd>

            <dt id="Reload">Reload</dt>
            <dd>
                <h2>The 'reload' command</h2>

                <p>

                This command is used to send a sighup to the persistent runner, forcing it to reload anything that was preloaded.

                <p>

                This is normally not necessary as yath will watch files for
                changes and reload as necessary (excluding any module that
                changed when it does).

                <p>

                The benefit of this is that it will clear the blacklist
                allowing all preloads to load once more.

                <script class="output">
$ yath reload

Sending SIGHUP to 31625
                </script>
            </dd>

            <dt id="Stop">Stop</dt>
            <dd>
                <h2>The 'stop' command</h2>

                This is used to stop a persistent runner. Output logs will also
                be rendered.

                <script class="output">
$ yath stop                                                                                                                         [~/projects/Moose-2.2006]

STDOUT LOG:
========================
31625 (default) Ready to run tests...
31625 (default) Waiting for currently running jobs to complete before exiting for restart...
31625 (default) Waiting for currently running jobs to complete before respawning...
31625 (default) Ready to run tests...
31625 Deleting /path/to/.yath-persist.json

========================

STDERR LOG:
========================
31625 (default) Runner cought SIGHUP, reloading...
31625 (default) Runner cought SIGHUP, reloading...

========================
                </script>
            </dd>
        </dl>
    </dd>

    <dt id="Plugins">Plugins</dt>
    <dd>
        <h2>Yath Plugins</h2>

        Plugins allow you to add custom options to commands or modify how files
        are found and processed. They must subclass App::Yath::Plugin.

        <p>

        You can use a plugin with the '-p' option:

        <script class="output">yath -pMyPlugin</script>

        The above will load the App::Yath::Plugin::MyPlugin module. You can specify a fully qualified module name by prefixing it with a +.

        <p>

        Available hooks:
        <script class="code">
            sub options {}               # Add cli options
            sub pre_init {}              # Run early
            sub post_init {}             # Run later
            sub find_files {}            # Find more test files
            sub block_default_search {}  # Prevent default test file search
            sub claim_file {}            # Claim a file that was found by default
        </script>
    </dd>

    <dt id="Speed">Speed</dt>
    <dd>
        <h2>Performance Example</h2>

        This table compares the performance of 'prove' and 'yath' when running
        the test suite for Moose-2.2006. All times were taken on the same
        system around the same time.

        <p>

        <table width="100%">
            <tr><th width="150">command         </th><th>Wallclock</th><th width="150">CPU Time</th><th>Notes</th></tr>
            <tr><td>prove -r t      </td><td align="center">96s   </td><td align="center">097.74</td><td>Just prove</td></tr>
            <tr><td>prove -r t -j3  </td><td align="center">49s   </td><td align="center">145.22</td><td>prove with 3 procs</td></tr>
            <tr><td>&nbsp;</td></tr>
            <tr><td>yath            </td><td align="center">97.97s</td><td align="center">097.63</td><td>Just yath</td></tr>
            <tr><td>yath -j3        </td><td align="center">50.17s</td><td align="center">146.66</td><td>Yath with 3 procs</td></tr>
            <tr><td>yath -PMoose    </td><td align="center">40.92s</td><td align="center">040.22</td><td>Yath with Moose preloaded</td></tr>
            <tr><td>yath -j3 -PMoose</td><td align="center">20.05s</td><td align="center">055.68</td><td>Yath with 3 procs and Moose preloaded</td></tr>
        </table>

        <p>

        <ul>
            <li>Lower is better</li>
            <li>Prove is not as precise when listing wallclock time</li>
            <li>yath is recursive by default and does not need the <i>-r</i> argument</li>
            <li>Arguments to find the correct lib paths ommited for simplicity</li>
        </ul>
    </dd>

    <dt id="Misc">Misc</dt>
    <dd>
        <dl class="sub_list">
            <dt id="Stream">Stream</dt>
            <dd>
                <h2>The Stream Formatter</h2>

                By default the harness will tell tests using Test2 to use
                Test2::Formatter::Stream. This formatter outputs all events to
                a jsonl file. In addition this formatter prints a timestamp to
                both STDOUT and STDERR on every event, which allows the 3 feeds
                to be synchronized later.

                <p>

                <b>Example output:</b>

                <p>

                events.jsonl:
                <script class="code_js">
{"assert_count":null,"event_id":"event-1","stream_id":1,"times":[0.07,0.0,0.0,0.0],"facet_data":{"control":{"encoding":"utf8"}},"stamp":1508945033.72702}
{"times":[0.07,0.0,0.0,0.0],"stream_id":2,"facet_data":{"trace":{"cid":"C1","pid":16217,"nested":0,"tid":0,"buffered":0,"frame":["main","t/bad_test.t",3,"Test2::Tools::Basic::ok"],"hid":"16217~0~1"},"assert":{"no_debug":1,"details":"A passing test","pass":1},"control":{},"about":{"package":"Test2::Event::Ok"}},"stamp":1508945033.72718,"assert_count":1,"event_id":"event-2"}
{"assert_count":2,"event_id":"event-3","times":[0.07,0.0,0.0,0.0],"stream_id":3,"facet_data":{"about":{"package":"Test2::Event::Ok"},"control":{},"trace":{"cid":"C2","pid":16217,"nested":0,"buffered":0,"tid":0,"frame":["main","t/bad_test.t",5,"Test2::Tools::Compare::is"],"hid":"16217~0~1"},"assert":{"pass":0,"details":"A Failing test","no_debug":1}},"stamp":1508945033.72814}
{"facet_data":{"info":[{"details":"Failed test 'A Failing test'\nat t/bad_test.t line 5.\n","debug":1,"tag":"DIAG"}],"trace":{"frame":["main","t/bad_test.t",5,"Test2::Tools::Compare::is"],"hid":"16217~0~1","pid":16217,"nested":0,"cid":"C2","tid":0,"buffered":0},"about":{"package":"Test2::Event::Diag"},"control":{}},"stamp":1508945033.72825,"times":[0.07,0.0,0.0,0.0],"stream_id":4,"assert_count":2,"event_id":"event-4"}
{"facet_data":{"control":{},"about":{"package":"Test2::Event::Diag"},"info":[{"tag":"DIAG","debug":1,"details":"+------+-----+----+-------+\n| PATH | GOT | OP | CHECK |\n+------+-----+----+-------+\n| {a}  | 1   | eq | 2     |\n+------+-----+----+-------+"}],"trace":{"hid":"16217~0~1","frame":["main","t/bad_test.t",5,"Test2::Tools::Compare::is"],"buffered":0,"tid":0,"cid":"C2","nested":0,"pid":16217}},"stamp":1508945033.7283,"stream_id":5,"times":[0.07,0.0,0.0,0.0],"assert_count":2,"event_id":"event-5"}
{"times":[0.07,0.0,0.0,0.0],"stream_id":6,"stamp":1508945033.72838,"facet_data":{"about":{"package":"Test2::Event::Ok"},"control":{},"assert":{"no_debug":1,"pass":1,"details":"final passing test"},"trace":{"frame":["main","t/bad_test.t",7,"Test2::Tools::Basic::ok"],"hid":"16217~0~1","pid":16217,"nested":0,"cid":"C3","tid":0,"buffered":0}},"event_id":"event-6","assert_count":3}
{"facet_data":{"trace":{"tid":0,"buffered":0,"cid":"C4","nested":0,"pid":16217,"hid":"16217~0~1","frame":["main","t/bad_test.t",9,"Test2::Tools::Basic::done_testing"]},"plan":{"count":3},"control":{"terminate":null},"about":{"package":"Test2::Event::Plan"}},"stamp":1508945033.72847,"times":[0.07,0.0,0.0,0.0],"stream_id":7,"assert_count":3,"event_id":"event-7"}
{"stamp":1508945033.72859,"facet_data":{"control":{},"about":{"package":"Test2::Event::Diag"},"info":[{"tag":"DIAG","details":"Seeded srand with seed '20171025' from local date.","debug":1}],"trace":{"pid":16217,"details":"Test2::API::Instance END Block finalization","tid":0,"frame":["Test2::API::Instance","/home/exodist/perl5/perlbrew/perls/main/lib/site_perl/5.26.0/Test2/API/Instance.pm",0,"Test2::API::Instance::END"]}},"stream_id":8,"times":[0.07,0.0,0.0,0.0],"event_id":"event-8","assert_count":3}
                </script>

                <div style="display: block; float: left; max-width: 40%">
                stdout:
                <script class="output">
T2-HARNESS-ESYNC: 1
T2-HARNESS-ESYNC: 2
This is a print to STDOUT
T2-HARNESS-ESYNC: 3
T2-HARNESS-ESYNC: 4
T2-HARNESS-ESYNC: 5
T2-HARNESS-ESYNC: 6
T2-HARNESS-ESYNC: 7
T2-HARNESS-ESYNC: 8
                </script>
                </div>

                <div style="display: block; float: left; max-width: 40%">
                stderr:
                <script class="output">
T2-HARNESS-ESYNC: 1
T2-HARNESS-ESYNC: 2
T2-HARNESS-ESYNC: 3
T2-HARNESS-ESYNC: 4
T2-HARNESS-ESYNC: 5
This is a print to STDERR
T2-HARNESS-ESYNC: 6
T2-HARNESS-ESYNC: 7
T2-HARNESS-ESYNC: 8
                </script>
                </div>
            </dd>

            <dt id="Model">Model</dt>
            <dd>
                <h2>Directory Structure</h2>

                <ul>
                    <li>Each 'run' has a temporary data dir</li>
                    <li>Each test has a directory under the 'run' dir</li>
                    <li>You can ask yath to keep the entire run dir with the '-k' option</li>
                </ul>

                The run directory is the 'model' layer. As well anything that
                processes a log will treat the log as its model layer.

                <script class="output">
                $ yath -k
                ...
                Keeping work dir: /dev/shm/yath-test-16215-ddXWdX44
                </script>

                <div style="display: block; float: left; max-width: 40%">
                Files in the run dir:
                <script class="output">
                    $ ls -1 /path/to/rundir/

                    1/
                    2/
                    3/
                    complete
                    error.log
                    jobs.jsonl
                    output.log
                    PID
                    queue.jsonl
                    ready
                    run.json
                </script>
                </div>

                <div style="display: block; float: left; max-width: 40%">
                Files in a test dir:
                <script class="output">
                    $ ls -1 /path/to/rundir/1

                    events.jsonl
                    exit
                    file
                    start
                    stderr
                    stdin
                    stdout
                    tmp/
                </script>
                </div>

            </dd>

            <dt id="View">View</dt>
            <dd>
                <h2>The App::Yath namespace</h2>

                <p>

                This namespace is where the code for the 'yath' script lives.

                <p>

                Code in this namespace should be UX, this is essentially the view layer.
            </dd>

            <dt id="Controller">Controller</dt>
            <dd>
                <h2>The Test2::Harness::Namespace</h2>

                <p>

                This namespace is where logic for running and processing tests goes.

                <p>

                This is essentially the controller layer.
            </dd>

            <dt id="TAP">TAP</dt>
            <dd>
                <h2>TAP Parsing</h2>

                </p>

                Test2::Harness has a TAP parser that is uses when the Stream formatter is not used, or when it detects TAP events.

                <p>

                Unlike prove/Test::Harness this parser WILL parse subtests.

                <p>

                The TAP parser is line based, the subtests are re-assembled and verified after parsing.
            </dd>
        </dl>
    </dd>
</dl>
